{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n6YdwR-iZHw"
      },
      "source": [
        "# 멋진 작사가 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT0E-irJiPL5"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQToRNeqqm9t"
      },
      "source": [
        "## Step 1. 데이터 다운로드\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJGJ7zfaq_Hs"
      },
      "source": [
        "## Step 2. 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0Zneoieq25D"
      },
      "outputs": [],
      "source": [
        "txt_file_path = '/content/drive/MyDrive/AIFFEL/Exp/05/data/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yULvHQIqssLN",
        "outputId": "b6e27086-233d-4e85-eb13-a025857d5a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples: ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\"]\n"
          ]
        }
      ],
      "source": [
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\", raw_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKKii75PtDs7"
      },
      "source": [
        "## Step 3. 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvOWWNsitzP2"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                   # 소문자로 바꾸고, 양쪽 공백 제거\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)   # 특수문자 양쪽에 공백 추가\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # 여러 개의 공백은 하나의 공백으로 수정\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 수정\n",
        "    sentence = sentence.strip()                           # 양쪽 공백 제거\n",
        "    sentence = '<start> ' + sentence + ' <end>'           # 문장 시작에는 <start>, 끝에는 <end> 추가\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Imw1H_ZvH_g",
        "outputId": "387dbcd4-7737-48f4-c9b5-3ac5f374c3bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# 정제된 문장을 담을 리스트\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkqA9hyit-j8"
      },
      "source": [
        "\n",
        "\n",
        "*   토큰의 개수가 15개를 넘어가는 경우 학습 데이터에서 제외하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fUZ0hqDvioW",
        "outputId": "902191ca-1c1d-4ea4-912a-15ae91abc494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 306  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2  23  77 ...   0   0   0]\n",
            " [  2  42  26 ...   0   0   0]\n",
            " [  2  23  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9717e45c90>\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000,\n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')  # 토큰 최대 개수 15개 제한\n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4kxYe5B4fAO"
      },
      "source": [
        "## Step 4. 평가 데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qPPSAsu3Gci"
      },
      "outputs": [],
      "source": [
        "src_input = tensor[:, :-1]\n",
        "tgt_input = tensor[:, 1:]  \n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, random_state=4, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czI0mlCp7KL2"
      },
      "source": [
        "## Step 5. 인공지능 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me4qQZY97d1s"
      },
      "source": [
        "\n",
        "\n",
        "*    10 Epoch 안에 loss 값이 2.2 이하로 나오는 모델 설계하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_train)\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptOOIjUa-OVI",
        "outputId": "efb0ee19-086e-4417-e5a1-5dc108566498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRiToZM38No3"
      },
      "outputs": [],
      "source": [
        "# 모델 생성 (Subclassing API)\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 512\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q92davq8hLW",
        "outputId": "ad3fe762-806e-4b08-ca4c-fb230a4aa187"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-1.63181336e-04,  1.95889152e-06,  9.15886631e-05, ...,\n",
              "          4.12018504e-04, -1.21971156e-04, -5.02773328e-05],\n",
              "        [-9.52694973e-06,  4.46697231e-06,  2.99327017e-04, ...,\n",
              "          5.25289157e-04, -6.97285577e-04, -2.75562226e-04],\n",
              "        [-1.14588147e-04,  2.45886855e-04,  2.28644349e-05, ...,\n",
              "          4.88593709e-04, -9.08439513e-04, -4.44687699e-04],\n",
              "        ...,\n",
              "        [ 1.25827733e-03,  1.97178521e-03,  1.48821482e-03, ...,\n",
              "          2.50645680e-04, -1.70636515e-04,  4.23447811e-04],\n",
              "        [ 1.15320657e-03,  1.69427041e-03,  1.56848878e-03, ...,\n",
              "         -9.20374296e-05,  1.53977307e-04,  4.99512302e-04],\n",
              "        [ 1.35645247e-03,  1.12317037e-03,  1.32821396e-03, ...,\n",
              "         -3.02157918e-04,  4.47147875e-04,  5.41375601e-04]],\n",
              "\n",
              "       [[-2.59502558e-05,  6.39617356e-05,  4.27165389e-04, ...,\n",
              "         -2.66848765e-05, -2.41005328e-04, -2.11982187e-05],\n",
              "        [ 1.75172449e-04,  1.72862667e-04,  2.97117047e-04, ...,\n",
              "         -5.02872172e-06, -3.61408340e-04,  1.30878005e-04],\n",
              "        [ 3.53810028e-04,  4.32974251e-04,  5.47600794e-04, ...,\n",
              "         -3.11999247e-05, -3.32022493e-04, -5.36557636e-05],\n",
              "        ...,\n",
              "        [-8.41841043e-04,  1.22361537e-03,  1.07140676e-03, ...,\n",
              "         -2.08972953e-04,  1.56152598e-03, -5.29300014e-04],\n",
              "        [-6.34235272e-04,  1.25682575e-03,  7.63078278e-04, ...,\n",
              "         -1.37547438e-04,  1.50096091e-03, -9.64293140e-04],\n",
              "        [-2.11943159e-04,  7.39009178e-04,  4.75039706e-04, ...,\n",
              "         -8.86878115e-05,  1.48716674e-03, -1.28724286e-03]],\n",
              "\n",
              "       [[-1.63181336e-04,  1.95889152e-06,  9.15886631e-05, ...,\n",
              "          4.12018504e-04, -1.21971156e-04, -5.02773328e-05],\n",
              "        [-3.72611976e-05,  2.30071455e-04,  9.86741943e-05, ...,\n",
              "          8.10594589e-04, -5.01238042e-04, -1.81537180e-05],\n",
              "        [ 1.92332314e-04,  2.70314689e-04,  1.08514883e-04, ...,\n",
              "          8.63109133e-04, -1.15474872e-03, -6.85302075e-06],\n",
              "        ...,\n",
              "        [-9.22117615e-04, -1.75343000e-03,  2.86565092e-03, ...,\n",
              "          4.07542568e-04, -2.48756580e-04, -1.94026413e-03],\n",
              "        [-7.67338206e-04, -2.19339272e-03,  3.48356599e-03, ...,\n",
              "          3.90105590e-04, -2.71985627e-04, -1.79984048e-03],\n",
              "        [-5.71890734e-04, -2.59841606e-03,  3.96279665e-03, ...,\n",
              "          3.91312235e-04, -2.87566829e-04, -1.59603765e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.63181336e-04,  1.95889152e-06,  9.15886631e-05, ...,\n",
              "          4.12018504e-04, -1.21971156e-04, -5.02773328e-05],\n",
              "        [-3.59603204e-04, -2.95736099e-05,  1.56554670e-04, ...,\n",
              "          1.00793340e-03, -3.91180220e-04, -3.21647269e-04],\n",
              "        [-2.81636545e-04, -3.86105967e-05, -1.66017853e-05, ...,\n",
              "          1.02980272e-03, -5.01416856e-04, -8.52624245e-04],\n",
              "        ...,\n",
              "        [ 4.15028684e-04, -8.57653271e-04,  3.06857657e-03, ...,\n",
              "         -1.57854520e-05,  1.89850689e-05, -7.26275553e-04],\n",
              "        [ 4.65746212e-04, -1.54322456e-03,  3.78965028e-03, ...,\n",
              "          2.30479054e-05,  1.78811897e-05, -8.64607806e-04],\n",
              "        [ 5.35772706e-04, -2.16279621e-03,  4.36258782e-03, ...,\n",
              "          6.61850208e-05,  9.21293395e-06, -9.21272906e-04]],\n",
              "\n",
              "       [[-1.63181336e-04,  1.95889152e-06,  9.15886631e-05, ...,\n",
              "          4.12018504e-04, -1.21971156e-04, -5.02773328e-05],\n",
              "        [-2.88492010e-04, -2.17127104e-04,  3.00739281e-04, ...,\n",
              "          8.88987212e-04,  1.66107115e-04,  1.54567417e-04],\n",
              "        [-3.13558849e-04, -2.36918597e-04,  6.25282642e-04, ...,\n",
              "          1.09181227e-03,  5.46797761e-04,  1.36219489e-04],\n",
              "        ...,\n",
              "        [ 8.24313611e-04, -2.76013953e-03,  1.59774581e-03, ...,\n",
              "         -1.53190995e-04,  2.98075138e-05,  5.57651685e-04],\n",
              "        [ 6.04298082e-04, -2.98530608e-03,  2.38632248e-03, ...,\n",
              "         -2.23724164e-05,  1.00399877e-04,  3.08944465e-04],\n",
              "        [ 4.33802779e-04, -3.23785748e-03,  3.12192575e-03, ...,\n",
              "          9.25221830e-05,  1.19184362e-04,  1.34956266e-04]],\n",
              "\n",
              "       [[-3.04167916e-05, -2.58101936e-04,  3.05087189e-04, ...,\n",
              "         -2.52349491e-06, -2.83408095e-04,  1.91308878e-04],\n",
              "        [ 3.37055419e-04, -4.96021414e-04,  3.37459031e-04, ...,\n",
              "         -1.72842207e-04, -4.64934157e-04,  2.97284394e-04],\n",
              "        [ 4.93824948e-04, -6.82045473e-04,  2.60277244e-04, ...,\n",
              "         -8.58454092e-04, -8.00902955e-04,  4.97231958e-04],\n",
              "        ...,\n",
              "        [-8.72306409e-05, -2.63738912e-04, -1.06761302e-03, ...,\n",
              "          9.20705847e-04, -9.79131437e-04,  1.84051110e-03],\n",
              "        [-4.63908713e-04,  3.08184477e-04, -8.16510583e-04, ...,\n",
              "          4.60444193e-04, -4.74770262e-04,  1.61649333e-03],\n",
              "        [-9.70926951e-04,  8.62026005e-04, -4.62052849e-04, ...,\n",
              "         -1.39179640e-04,  1.92606676e-04,  1.48402993e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "# 모델 빌드\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHmPrQ27_mL0",
        "outputId": "9af50651-6b66-4f98-aa97-bb6c08f05858"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "549/549 [==============================] - 506s 916ms/step - loss: 3.6614\n",
            "Epoch 2/10\n",
            "549/549 [==============================] - 487s 886ms/step - loss: 3.1549\n",
            "Epoch 3/10\n",
            "549/549 [==============================] - 487s 886ms/step - loss: 2.9582\n",
            "Epoch 4/10\n",
            "549/549 [==============================] - 488s 889ms/step - loss: 2.8124\n",
            "Epoch 5/10\n",
            "549/549 [==============================] - 489s 891ms/step - loss: 2.6852\n",
            "Epoch 6/10\n",
            "549/549 [==============================] - 487s 886ms/step - loss: 2.5666\n",
            "Epoch 7/10\n",
            "549/549 [==============================] - 474s 863ms/step - loss: 2.4550\n",
            "Epoch 8/10\n",
            "549/549 [==============================] - 495s 901ms/step - loss: 2.3495\n",
            "Epoch 9/10\n",
            "549/549 [==============================] - 494s 899ms/step - loss: 2.2489\n",
            "Epoch 10/10\n",
            "549/549 [==============================] - 483s 879ms/step - loss: 2.1514\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9717967290>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MSGE6OXh78j0"
      },
      "outputs": [],
      "source": [
        "# 문장 생성기\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        predict = model(test_tensor) \n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
        "\n",
        "\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "zRsatMmqA9wA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "689d699e-ca9f-4a69-fc45-c5a387c8eae2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cn2WCcVwlC3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고"
      ],
      "metadata": {
        "id": "mZRFqZbhk-mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'언어 모델'이란 개념의 이해부터 시작해 자연어 데이터에 대한 전처리 과정, Subclassing 방식의 모델링, 최종 학습된 내용을 바탕으로 새 문장을 생성하기까지 NLP의 주요 프로세스를 실습해 볼 수 있어서 좋았다.\n",
        "\n",
        "그러나 아직 전처리 과정에 사용되는 regex 문법이 미숙하고, 모델 파라미터와 Embedding layer에 대한 이해도 완벽하지 못하다.\n",
        "\n",
        "현재는 10 epoch 내 loss 값이 *2.15* 로 나타나는 데 이를 효과적으로 더 낮추기 위해서 embedding size나 hidden size 외에 또 어느 부분을 수정해 볼 수 있는 지 궁금하다."
      ],
      "metadata": {
        "id": "Ut4xuxLklEHB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[E-05]LSTM.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
